{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Project: Create a Customer Segmentation Report for Arvato Financial Services\n",
    "\n",
    "In this project, you will analyze demographics data for customers of a mail-order sales company in Germany, comparing it against demographics information for the general population. You'll use unsupervised learning techniques to perform customer segmentation, identifying the parts of the population that best describe the core customer base of the company. Then, you'll apply what you've learned on a third dataset with demographics information for targets of a marketing campaign for the company, and use a model to predict which individuals are most likely to convert into becoming customers for the company. The data that you will use has been provided by our partners at Bertelsmann Arvato Analytics, and represents a real-life data science task.\n",
    "\n",
    "If you completed the first term of this program, you will be familiar with the first part of this project, from the unsupervised learning project. The versions of those two datasets used in this project will include many more features and has not been pre-cleaned. You are also free to choose whatever approach you'd like to analyzing the data rather than follow pre-determined steps. In your work on this project, make sure that you carefully document your steps and decisions, since your main deliverable for this project will be a blog post reporting your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import libraries here; add more as necessary\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "# from yellowbrick.cluster.elbow import kelbow_visualizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "# magic word for producing visualizations in notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %whos DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 0: Get to Know the Data\n",
    "\n",
    "There are four data files associated with this project:\n",
    "\n",
    "- `Udacity_AZDIAS_052018.csv`: Demographics data for the general population of Germany; 891 211 persons (rows) x 366 features (columns).\n",
    "- `Udacity_CUSTOMERS_052018.csv`: Demographics data for customers of a mail-order company; 191 652 persons (rows) x 369 features (columns).\n",
    "- `Udacity_MAILOUT_052018_TRAIN.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 982 persons (rows) x 367 (columns).\n",
    "- `Udacity_MAILOUT_052018_TEST.csv`: Demographics data for individuals who were targets of a marketing campaign; 42 833 persons (rows) x 366 (columns).\n",
    "\n",
    "Each row of the demographics files represents a single person, but also includes information outside of individuals, including information about their household, building, and neighborhood. Use the information from the first two files to figure out how customers (\"CUSTOMERS\") are similar to or differ from the general population at large (\"AZDIAS\"), then use your analysis to make predictions on the other two files (\"MAILOUT\"), predicting which recipients are most likely to become a customer for the mail-order company.\n",
    "\n",
    "The \"CUSTOMERS\" file contains three extra columns ('CUSTOMER_GROUP', 'ONLINE_PURCHASE', and 'PRODUCT_GROUP'), which provide broad information about the customers depicted in the file. The original \"MAILOUT\" file included one additional column, \"RESPONSE\", which indicated whether or not each recipient became a customer of the company. For the \"TRAIN\" subset, this column has been retained, but in the \"TEST\" subset it has been removed; it is against that withheld column that your final predictions will be assessed in the Kaggle competition.\n",
    "\n",
    "Otherwise, all of the remaining columns are the same between the three data files. For more information about the columns depicted in the files, you can refer to two Excel spreadsheets provided in the workspace. [One of them](./DIAS Information Levels - Attributes 2017.xlsx) is a top-level list of attributes and descriptions, organized by informational category. [The other](./DIAS Attributes - Values 2017.xlsx) is a detailed mapping of data values for each feature in alphabetical order.\n",
    "\n",
    "In the below cell, we've provided some initial code to load in the first two datasets. Note for all of the `.csv` data files in this project that they're semicolon (`;`) delimited, so an additional argument in the [`read_csv()`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html) call has been included to read in the data properly. Also, considering the size of the datasets, it may take some time for them to load completely.\n",
    "\n",
    "You'll notice when the data is loaded in that a warning message will immediately pop up. Before you really start digging into the modeling and analysis, you're going to need to perform some cleaning. Take some time to browse the structure of the data and look over the informational spreadsheets to understand the data values. Make some decisions on which features to keep, which features to drop, and if any revisions need to be made on data formats. It'll be a good idea to create a function with pre-processing steps, since you'll need to clean all of the datasets before you work with them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine azdias dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load in the data\n",
    "azdias = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_AZDIAS_052018.csv', sep=';')\n",
    "\n",
    "azdias.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'Shape', azdias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "azdias.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "azdias.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine customers dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data\n",
    "customers = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_CUSTOMERS_052018.csv', sep=';')\n",
    "\n",
    "customers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Shape', customers.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "customers.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "customers.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further consideration of datasets, categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Look at extra columns\n",
    "\n",
    "extra = ['CUSTOMER_GROUP', 'ONLINE_PURCHASE', 'PRODUCT_GROUP']\n",
    "customers['ONLINE_PURCHASE'] = customers['ONLINE_PURCHASE'].astype(str)\n",
    "customers['ONLINE_PURCHASE'].replace({'0': 'No', '1': 'Yes'}, inplace=True)\n",
    "\n",
    "print(customers[extra].dtypes)\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(21,7))\n",
    "for i in range(3):\n",
    "    customers[extra[i]].value_counts().plot(kind='bar', ax=ax[i], width=.9)\n",
    "    ax[i].legend(loc='best')\n",
    "ax[1].set_title('Groups of customers, purchases, and products', fontsize=15, y=1.03)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Save this additional info in another customers_extra DF\n",
    "\n",
    "extra = ['CUSTOMER_GROUP', 'ONLINE_PURCHASE', 'PRODUCT_GROUP']\n",
    "customers_extra = customers[extra]\n",
    "customers.drop(extra, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols_azdias = azdias.select_dtypes(include='object').columns\n",
    "cat_cols_customers = customers.select_dtypes(include='object').columns\n",
    "set(cat_cols_customers).difference(set(cat_cols_azdias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Plot up to top-20\n",
    "\n",
    "plt.rc('xtick', labelsize=12)\n",
    "\n",
    "for column in cat_cols_azdias:\n",
    "    if column != 'EINGEFUEGT_AM':\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(21,7))\n",
    "        azdias[column].value_counts().iloc[:20].plot(kind='bar', ax=ax[0], width=0.9, cmap='GnBu_r')\n",
    "        customers[column].value_counts().iloc[:20].plot(kind='bar', ax=ax[1], width=0.9, cmap='GnBu_r')\n",
    "        plt.suptitle(column, fontsize=15)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(azdias['EINGEFUEGT_AM'].dtype)\n",
    "print(azdias['EINGEFUEGT_AM'].unique())\n",
    "azdias['EINGEFUEGT_AM'].value_counts().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(customers['EINGEFUEGT_AM'].dtype)\n",
    "print(customers['EINGEFUEGT_AM'].unique())\n",
    "customers['EINGEFUEGT_AM'].value_counts().describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further consideration of datasets, distribution of numerical variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As my project reviewer suggested, I examined the distribution of the corresponding column values in both AZDIAS and CUSTOMERS datasets. As a result of manual comparing the numbers on figures, a list was created with columns in which distributions aren't similar. In further analysis, these columns will be retained from removing columns with multiple missing values. It's my interpretation that if the same columns from both DataFrames have different distributions, we can use this additional information in our clustering model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "columns_ = azdias.select_dtypes(include='int').columns.tolist()\n",
    "\n",
    "# All figures gets a lot of space, so only one example \"D19_KONSUMTYP_MAX\" column is plotted\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(21,7))\n",
    "azdias['D19_KONSUMTYP_MAX'].plot.hist(ax=ax[0], cmap='GnBu_r')\n",
    "customers['D19_KONSUMTYP_MAX'].plot.hist(ax=ax[1], cmap='GnBu_r')\n",
    "plt.suptitle('D19_KONSUMTYP_MAX', fontsize=15)\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "# Plot columns whichdtype is integer\n",
    "for column in columns_:\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(21,7))\n",
    "    azdias[column].plot.hist(ax=ax[0], cmap='GnBu_r')\n",
    "    customers[column].plot.hist(ax=ax[1], cmap='GnBu_r')\n",
    "    plt.suptitle(column, fontsize=15)\n",
    "    plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "columns_ = azdias.select_dtypes(include='float').columns.tolist()\n",
    "\n",
    "# All figures gets a lot of space, so only one example \"D19_SOZIALES\" column is plotted\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(21,7))\n",
    "azdias['D19_SOZIALES'].plot.hist(ax=ax[0], cmap='GnBu_r')\n",
    "customers['D19_SOZIALES'].plot.hist(ax=ax[1], cmap='GnBu_r')\n",
    "plt.suptitle('D19_SOZIALES', fontsize=15)\n",
    "plt.show()\n",
    "\n",
    "'''\n",
    "# Plot columns whichdtype is float\n",
    "for column in columns_:\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(21,7))\n",
    "    azdias[column].plot.hist(ax=ax[0], cmap='GnBu_r')\n",
    "    customers[column].plot.hist(ax=ax[1], cmap='GnBu_r')\n",
    "    plt.suptitle(column, fontsize=15)\n",
    "    plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cols_to_keep = ['D19_KONSUMTYP_MAX', 'FINANZ_ANLEGER', 'FINANZ_MINIMALIST', 'FINANZTYP', 'HEALTH_TYP', \n",
    "                'PRAEGENDE_JUGENDJAHRE', 'SEMIO_MAT', 'SEMIO_VERT', 'SHOPPER_TYP', 'ZABEOTYP', 'ANREDE_KZ',\n",
    "                'AKT_DAT_KL', 'ALTER_HH', 'CJT_GESAMTTYP', 'CJT_TYP_1', 'CJT_TYP_2', 'D19_KONSUMTYP', 'D19_SOZIALES',\n",
    "                'EINGEZOGENAM_HH_JAHR', 'GFK_URLAUBERTYP', 'HH_EINKOMMEN_SCORE', 'KBA05_ANTG1', 'KBA05_ANTG2',\n",
    "                'KBA05_BAUMAX', 'KBA05_CCM4', 'KBA05_GBZ', 'KBA05_MAXAH', 'KBA05_MOD1', 'KBA05_MOD8', 'KBA05_SEG9',\n",
    "                'LP_STATUS_FEIN', 'LP_STATUS_GROB', 'MOBI_REGIO', 'ORTSGR_KLS9', 'RT_KEIN_ANREIZ', 'RT_UEBERGROESSE',\n",
    "                'VHA', 'VK_DHT4A', 'VK_DISTANZ', 'VK_ZG11']\n",
    "\n",
    "for column in cols_to_keep:\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(21,7))\n",
    "    azdias[column].plot.hist(ax=ax[0], cmap='GnBu_r')\n",
    "    customers[column].plot.hist(ax=ax[1], cmap='GnBu_r')\n",
    "    plt.suptitle(column, fontsize=15)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Work with DIAS Information Levels and Values / 2017"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load in a top-level list of attributes and descriptions, organized by informational category file\n",
    "\n",
    "levels_df = pd.read_excel('DIAS Information Levels - Attributes 2017.xlsx', skiprows=1)\n",
    "levels_df = levels_df[['Information level', 'Attribute', 'Description']]\n",
    "levels_df.loc[0, 'Information level'] = 'Person'\n",
    "levels_df.loc[88:96, 'Information level'] = 'Microcell (RR3_ID)'\n",
    "levels_df['Information level'] = levels_df['Information level'].fillna(method='ffill')\n",
    "levels_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levels_df['Information level'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# load in a detailed mapping of data values for each feature in alphabetical order\n",
    "\n",
    "values_df = pd.read_excel('DIAS Attributes - Values 2017.xlsx', skiprows=1)\n",
    "values_df.drop('Unnamed: 0', axis=1, inplace=True)\n",
    "values_df = values_df.fillna(method='ffill')\n",
    "values_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "details = values_df.merge(levels_df, on=['Attribute', 'Description'], how='left')\n",
    "details.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "details.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "details.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of all values and unique Attributes in DIAS data\n",
    "details.shape, details.Attribute.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "details.Meaning.value_counts().iloc[:50].plot.bar(figsize=(21,7), width=.9)\n",
    "plt.title('Distribution of DIAS Attributes Values', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following part such values as \"unknown\" or \"unknown / no main age detectable\", etc. will be replaced with NaNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replace unknown values in azdias and customers with NaN using details DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get list of Attribute - Value where \"unknown\" or \"no transaction(s) known\" is present in Meaning\n",
    "\n",
    "details['meaning'] = details.Meaning.str.lower().astype(str)\n",
    "unknown = details[(details.meaning.str.contains('unknown')) |\n",
    "                  (details.meaning.isin(['no transactions known', 'no transaction known']))][['Attribute', 'Value']]\n",
    "unknown.set_index('Attribute', inplace=True)\n",
    "unknown.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def replace_unknown_with_nan(df, unknown):\n",
    "\n",
    "    cols_df = df.columns\n",
    "    for column in unknown.index:\n",
    "        if column in cols_df:\n",
    "            col_values = df[column].unique().tolist()\n",
    "            unknown_vals = unknown.loc[column]['Value']\n",
    "            for val in col_values:\n",
    "                if isinstance(unknown_vals, int):\n",
    "                    if val == unknown_vals:\n",
    "                        df[column] = df[column].replace(val, np.nan)\n",
    "                else:\n",
    "                    if str(val) in unknown_vals.split():\n",
    "                        df[column] = df[column].replace(val, np.nan)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply function replace_unknown_with_nan to azdias dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "azdias.isnull().sum().sort_values(ascending=False).iloc[:40].plot.bar(figsize=(21,7), width=.9, cmap='GnBu_r')\n",
    "plt.title('Missing values before editting', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "azdias = replace_unknown_with_nan(azdias, unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "azdias.isnull().sum().sort_values(ascending=False).iloc[:40].plot.bar(figsize=(21,7), width=.9, cmap='GnBu_r')\n",
    "plt.title('Missing values after editting', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply function replace_unknown_with_nan to customers dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "customers.isnull().sum().sort_values(ascending=False).iloc[:50].plot.bar(figsize=(21,7), width=.9, cmap='GnBu_r')\n",
    "plt.title('Missing values before editting', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = replace_unknown_with_nan(customers, unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "customers.isnull().sum().sort_values(ascending=False).iloc[:40].plot.bar(figsize=(21,7), width=.9, cmap='GnBu_r')\n",
    "plt.title('Missing values after editting', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop columns from azdias and customers where there's no corresponding value in Attribute in details DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Differences and similarities in azdias and details DF\n",
    "\n",
    "common_cols = set(azdias.columns.tolist()).intersection(set(details.Attribute.unique().tolist()))\n",
    "different_cols = set(azdias.columns.tolist()).symmetric_difference(set(details.Attribute.unique().tolist()))\n",
    "different_azdias_cols = set(azdias.columns.tolist()).difference(set(details.Attribute.unique().tolist()))\n",
    "different_details_cols = set(details.Attribute.tolist()).difference(set(azdias.columns.unique().tolist()))\n",
    "\n",
    "print('Same for both azdias and details', len(common_cols))\n",
    "print('Different for both azdias and details', len(different_cols))\n",
    "print('Different for azdias', len(different_azdias_cols))\n",
    "print('Different for details', len(different_details_cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Differences and similarities in customers and details DF\n",
    "\n",
    "common_cols = set(customers.columns.tolist()).intersection(set(details.Attribute.unique().tolist()))\n",
    "different_cols = set(customers.columns.tolist()).symmetric_difference(set(details.Attribute.unique().tolist()))\n",
    "different_customers_cols = set(customers.columns.tolist()).difference(set(details.Attribute.unique().tolist()))\n",
    "different_details_cols = set(details.Attribute.tolist()).difference(set(customers.columns.unique().tolist()))\n",
    "\n",
    "print('Same for both customers and details', len(common_cols))\n",
    "print('Different for both customers and details', len(different_cols))\n",
    "print('Different for customers', len(different_azdias_cols))\n",
    "print('Different for details', len(different_details_cols))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that D19 columns with \"RZ\" in the end of column names in DIAS Attributes DF (\"RZ\" stands for \"Herzogtum Lauenburg\" region in Germany) means the same as the corresponding columns of total population. So, the unknown or missing values could also be replaced with NaN.\n",
    "The same could be applied to columns CAMEO_INTL_2015 (azdias DF) and CAMEO_DEUINTL_2015 () are the same (and probably \"DEU\" means \"Deutsche Eislauf-Union\", but it's not exact)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details[details.Attribute == 'D19_BANKEN_GROSS_RZ'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "details.Attribute = details.Attribute.str.replace('_RZ', '')\n",
    "details[details.Attribute == 'D19_BANKEN_GROSS'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias = replace_unknown_with_nan(azdias, unknown)\n",
    "\n",
    "azdias.isnull().sum().sort_values(ascending=False).iloc[:50].plot.bar(figsize=(21,7), width=.9, cmap='GnBu_r')\n",
    "plt.title('Missing values after editting', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "customers = replace_unknown_with_nan(customers, unknown)\n",
    "\n",
    "customers.isnull().sum().sort_values(ascending=False).iloc[:50].plot.bar(figsize=(21,7), width=.9, cmap='GnBu_r')\n",
    "plt.title('Missing values after editting', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert categorical columns to numeric where possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def object_toNumeric(df):\n",
    "\n",
    "    cat_cols = df.select_dtypes(include='object').columns\n",
    "    for column in cat_cols:\n",
    "        print(column)\n",
    "        unqiue_vals = df[column].unique()\n",
    "        print('Unique values in', column, unqiue_vals, '\\n')\n",
    "        if ('X' in unqiue_vals) or ('XX' in unqiue_vals):\n",
    "            df[column] = df[column].replace({'X': np.nan, 'XX': np.nan})\n",
    "            df[column] = pd.to_numeric(df[column], errors='coerse')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "azdias = object_toNumeric(azdias)\n",
    "azdias.select_dtypes(include='object').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "customers = object_toNumeric(customers)\n",
    "customers.select_dtypes(include='object').head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert categorical columns to datetime where possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def object_toDatetime(df, column):\n",
    "\n",
    "    df[column] = pd.to_datetime(df[column])\n",
    "    df['YEAR'] = df[column].dt.year\n",
    "    df['MONTH'] = df[column].dt.month\n",
    "    df.drop(column, axis=1, inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "azdias = object_toDatetime(azdias, 'EINGEFUEGT_AM')\n",
    "azdias[['YEAR', 'MONTH']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "customers = object_toDatetime(customers, 'EINGEFUEGT_AM')\n",
    "customers[['YEAR', 'MONTH']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nan_azdias = azdias.isnull().sum()\n",
    "nan_azdias = nan_azdias.sort_values(ascending=False)\n",
    "nan_azdias_cols = nan_azdias[nan_azdias > 0]\n",
    "print('Number of columns with NaNs is', len(nan_azdias_cols))\n",
    "proportion_azdias = nan_azdias_cols / azdias.shape[0]\n",
    "print(proportion_azdias[proportion_azdias > .9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nan_customers = customers.isnull().sum()\n",
    "nan_customers = nan_customers.sort_values(ascending=False)\n",
    "nan_customers_cols = nan_customers[nan_customers > 0]\n",
    "print('Number of columns with NaNs is', len(nan_customers_cols))\n",
    "proportion_customers = nan_customers_cols / customers.shape[0]\n",
    "print(proportion_customers[proportion_customers > .9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The calculated statistics for each feature in demographic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_feat_info(df):\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'value_count': [df[column].count() for column in df.columns], \n",
    "        'value_distinct' : [df[column].unique().shape[0] for column in df.columns],  \n",
    "        'num_nans':  [df[column].isnull().sum() for column in df.columns], \n",
    "        'percent_nans' : [round(df[column].isnull().sum()/df[column].shape[0], 3) for column in df.columns],\n",
    "    },  index = df.columns)    \n",
    "\n",
    "feat_info_azdias =  build_feat_info(azdias)\n",
    "feat_info_azdias.sort_values(by=['percent_nans'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feat_info_customers =  build_feat_info(customers)\n",
    "feat_info_customers.sort_values(by=['percent_nans'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete columns with more than 90% missing values\n",
    "\n",
    "cols_to_drop = nan_azdias_cols[nan_azdias_cols / azdias.shape[0] > .9].index.tolist()\n",
    "cols_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias.drop(cols_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_drop = nan_customers_cols[nan_customers_cols / customers.shape[0] > .9].index.tolist()\n",
    "cols_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.drop(cols_to_drop, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Column which has a little bit less missing values in azdias than in customers DF\n",
    "\n",
    "difference = (set(proportion_customers[proportion_customers > .9].index)\n",
    "                  .difference(set(proportion_azdias[proportion_azdias > .9].index)))\n",
    "print(difference)\n",
    "print(proportion_azdias[difference])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias.drop(difference, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize distribution of missing values in top-30 columns in azdias\n",
    "\n",
    "nan_azdias = azdias.isnull().sum()\n",
    "nan_azdias = nan_azdias.sort_values(ascending=False)\n",
    "nan_azdias_cols = nan_azdias[nan_azdias > 0]\n",
    "\n",
    "nan_azdias_cols.iloc[:30].plot(kind='bar', figsize=(21,7), width=.9, color='green')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Distribution of NaNs in azdias DataFrame', fontsize=15)\n",
    "plt.xlabel('Top-30 columns with NaNs')\n",
    "plt.ylabel('Number of NaNs')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualize distribution of missing values in top-30 columns in customers\n",
    "\n",
    "nan_customers = customers.isnull().sum()\n",
    "nan_customers = nan_customers.sort_values(ascending=False)\n",
    "nan_customers_cols = nan_customers[nan_customers > 0]\n",
    "\n",
    "nan_customers_cols.iloc[:30].plot(kind='bar', figsize=(21,7), width=.9, color='green')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Distribution of NaNs in customers DataFrame', fontsize=15)\n",
    "plt.xlabel('Top-30 columns with NaNs')\n",
    "plt.ylabel('Number of NaNs')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape after deleting columns with more than 90% missing values')\n",
    "print('Azdias', azdias.shape)\n",
    "print('Customers', customers.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop columns and create new ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column which represent sum of NaN in each row\n",
    "\n",
    "def create_num_missing_column(df):    \n",
    "    df['Num_missing'] = df.isnull().sum(axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias = create_num_missing_column(azdias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers = create_num_missing_column(customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For columns with NaN more than 10% and less 90% replace missing values with 0s, and non-missing with 1s\n",
    "\n",
    "def tranform_cols_with_missing(df, cols_to_keep):\n",
    "    nan_df = df.isnull().sum() / df.shape[0]\n",
    "    nan_cols = nan_df[(nan_df > .1) & (nan_df < .9)]\n",
    "    for column in nan_cols.index:\n",
    "        if column not in cols_to_keep:\n",
    "            df[column] = np.where(df[column].isnull(), 0, 1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "azdias = tranform_cols_with_missing(azdias, cols_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "customers = tranform_cols_with_missing(customers, cols_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For columns with NaN in cols_to_keep\n",
    "\n",
    "def missing_vals_in_cols_to_keep(df, cols_to_keep):\n",
    "    nan_df = df.isnull().sum() / df.shape[0]\n",
    "    nan_cols = nan_df[(nan_df > .1) & (nan_df < .9)]\n",
    "    for column in df[nan_cols.index]:\n",
    "        df[column] = df[column].replace(np.nan, -1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "azdias = missing_vals_in_cols_to_keep(azdias, cols_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "customers = missing_vals_in_cols_to_keep(customers, cols_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Work with azdias DF where # of missing values < 10%\n",
    "\n",
    "nan_azdias = azdias.isnull().sum() / azdias.shape[0]\n",
    "nan_cols = nan_azdias[(nan_azdias > 0) & (nan_azdias < .1)]\n",
    "print(nan_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace missing values with modes of correspondng columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for column in azdias[nan_cols.index]:\n",
    "    col_mode = azdias[column].mode().values[0]\n",
    "    azdias[column] = azdias[column].replace(np.nan, col_mode)\n",
    "\n",
    "# azdias.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Work with customers DF where # of missing values < 10%\n",
    "\n",
    "nan_customers = customers.isnull().sum() / customers.shape[0]\n",
    "nan_cols = nan_customers[(nan_customers > 0) & (nan_customers < .1)]\n",
    "print(nan_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "customers[nan_cols.index].plot.hist(figsize=(21,7), alpha=.5, bins=20)\n",
    "plt.legend(loc='best')\n",
    "plt.title('Histograms of columns with <10% missing values', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case less than 2% of answers are missing. Since, values in this columns are discrete, and distributions are more or less normal (except \"CJT_TYP_1\" column), missing values will be replaced with modes of correspondng columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for column in customers[nan_cols.index]:\n",
    "    col_mode = customers[column].mode().values[0]\n",
    "    customers[column] = customers[column].replace(np.nan, col_mode)\n",
    "\n",
    "# customers.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('# of missing in azdias', azdias.isnull().sum().sum())\n",
    "print('# of missing in customers', customers.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Whether all values in LNR column are unique in azdias DF -', len(azdias.LNR.unique()) == azdias.shape[0])\n",
    "print('Whether all values in LNR column are unique in customers DF - ',\n",
    "      len(customers.LNR.unique()) == customers.shape[0])\n",
    "\n",
    "print('# of common values in LNR column', len(set(customers.LNR).intersection(set(azdias.LNR))))\n",
    "print('# of different values in LNR column', len(set(customers.LNR).difference(set(azdias.LNR))))\n",
    "\n",
    "# So, we can drop LNR column in both datasets\n",
    "azdias.drop('LNR', axis=1, inplace=True)\n",
    "customers.drop('LNR', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create dummies from categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummies(df):\n",
    "\n",
    "    cat_cols = df.select_dtypes(include='object').columns\n",
    "    for column in cat_cols:\n",
    "        num_unique = len(df[column].value_counts())\n",
    "        if (num_unique == 2) or (num_unique == 3 and np.nan in df[column].value_counts()):\n",
    "            values = df[column].value_counts()\n",
    "            df[column] = df[column].replace({values.index[0]: 0, values.index[1]: 1})\n",
    "        else:\n",
    "            df = pd.concat([df.drop(column, axis=1), pd.get_dummies(df[column], drop_first=True)], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "azdias = create_dummies(azdias)\n",
    "azdias.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias.dtypes.value_counts().plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "customers = create_dummies(customers)\n",
    "customers.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customers.dtypes.value_counts().plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias.shape, customers.shape, customers_extra.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function with pre-processing steps to clean all of the datasets before work with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_demographic_data(df, unknown, cat_column='EINGEFUEGT_AM', cols_to_keep=cols_to_keep):\n",
    "    \n",
    "    ##############################\n",
    "    # The calculated statistics for each feature in demographic data\n",
    "    feat_info_df =  build_feat_info(df)\n",
    "    print(feat_info_df.sort_values(by=['percent_nans'], ascending=False))\n",
    "    \n",
    "    ##############################\n",
    "    # Replace unknown values with NaNs\n",
    "\n",
    "    cols_df = df.columns\n",
    "    for column in unknown.index:\n",
    "        if column in cols_df:\n",
    "            col_values = df[column].unique().tolist()\n",
    "            unknown_vals = unknown.loc[column]['Value']\n",
    "            for val in col_values:\n",
    "                if isinstance(unknown_vals, int):\n",
    "                    if val == unknown_vals:\n",
    "                        df[column] = df[column].replace(val, np.nan)\n",
    "                else:\n",
    "                    if str(val) in unknown_vals.split():\n",
    "                        df[column] = df[column].replace(val, np.nan)\n",
    "\n",
    "    \n",
    "    ##############################\n",
    "    # Convert categorical columns to numeric where possible\n",
    "    \n",
    "    cat_cols = df.select_dtypes(include='object').columns\n",
    "    for column in cat_cols:\n",
    "        unqiue_vals = df[column].unique()\n",
    "        if ('X' in unqiue_vals) or ('XX' in unqiue_vals):\n",
    "            df[column] = df[column].replace({'X': np.nan, 'XX': np.nan})\n",
    "            df[column] = pd.to_numeric(df[column], errors='coerse')\n",
    "    \n",
    "    ##############################\n",
    "    # Convert categorical columns to datetime where possible\n",
    "\n",
    "    df[cat_column] = pd.to_datetime(df[cat_column])\n",
    "    df['YEAR'] = df[cat_column].dt.year\n",
    "    df['MONTH'] = df[cat_column].dt.month\n",
    "    df.drop(cat_column, axis=1, inplace=True)\n",
    "    \n",
    "    ##############################\n",
    "    # Missing values\n",
    "    \n",
    "    nan_df = df.isnull().sum()\n",
    "    nan_df = nan_df.sort_values(ascending=False)\n",
    "    nan_df_cols = nan_df[nan_df > 0]\n",
    "    proportion_df = nan_df_cols / df.shape[0]\n",
    "    \n",
    "    # Delete columns with more than 90% missing values\n",
    "    cols_to_drop = nan_df_cols[nan_df_cols / df.shape[0] > .9].index.tolist()\n",
    "    df.drop(cols_to_drop, axis=1, inplace=True)\n",
    "    \n",
    "    ##############################\n",
    "    # Create column which represent sum of NaN in each row\n",
    "    df['Num_missing'] = df.isnull().sum(axis=1)\n",
    "    \n",
    "    ##############################\n",
    "    # For columns with NaN more than 10% and less 90% replace missing values with 0s, and non-missing with 1s\n",
    "    # (except cols_to_keep)\n",
    "    \n",
    "    nan_df = df.isnull().sum() / df.shape[0]\n",
    "    nan_cols = nan_df[(nan_df > .1) & (nan_df < .9)]\n",
    "    for column in nan_cols.index:\n",
    "        if column not in cols_to_keep:\n",
    "            print(column)\n",
    "            df[column] = np.where(df[column].isnull(), 0, 1)\n",
    "    \n",
    "    ##############################\n",
    "    # For columns with NaN in cols_to_keep\n",
    "    \n",
    "    nan_df = df.isnull().sum() / df.shape[0]\n",
    "    nan_cols = nan_df[(nan_df > .1) & (nan_df < .9)]\n",
    "    for column in df[nan_cols.index]:\n",
    "        df[column] = df[column].replace(np.nan, -1)\n",
    "    \n",
    "    ##############################\n",
    "    # LNR column    \n",
    "    print('Whether all values in LNR column are unique in azdias DF -', len(df.LNR.unique()) == df.shape[0])\n",
    "    df.drop('LNR', axis=1, inplace=True)\n",
    "    \n",
    "    ##############################\n",
    "    # Work with customers DF where # of missing values < 10%\n",
    "\n",
    "    nan_df = df.isnull().sum() / df.shape[0]\n",
    "    nan_cols = nan_df[(nan_df > 0) & (nan_df < .1)]\n",
    "    \n",
    "    for column in df[nan_cols.index]:\n",
    "        col_mode = df[column].mode().values[0]\n",
    "        df[column] = df[column].replace(np.nan, col_mode)\n",
    "    \n",
    "    ##############################\n",
    "    # Create dummies from categorical variables\n",
    "\n",
    "    cat_cols = df.select_dtypes(include='object').columns\n",
    "    for column in cat_cols:\n",
    "        num_unique = len(df[column].value_counts())\n",
    "        if (num_unique == 2) or (num_unique == 3 and np.nan in df[column].value_counts()):\n",
    "            values = df[column].value_counts()\n",
    "            df[column] = df[column].replace({values.index[0]: 0, values.index[1]: 1})\n",
    "        else:\n",
    "            df = pd.concat([df.drop(column, axis=1), pd.get_dummies(df[column], drop_first=True)], axis=1)\n",
    "    \n",
    "\n",
    "    \n",
    "    print('Final check of missing values', df.isnull().sum().sum())\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Customer Segmentation Report\n",
    "\n",
    "The main bulk of your analysis will come in this part of the project. Here, you should use unsupervised learning techniques to describe the relationship between the demographics of the company's existing customers and the general population of Germany. By the end of this part, you should be able to describe parts of the general population that are more likely to be part of the mail-order company's main customer base, and which parts of the general population are less so."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Standardize values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "azdias_cols = azdias.columns\n",
    "customers_cols = customers.columns\n",
    "\n",
    "scaler = StandardScaler()\n",
    "azdias_scaled = scaler.fit_transform(azdias)\n",
    "print('azdias done!')\n",
    "customers_scaled = scaler.transform(customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(azdias_scaled).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implement PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(.9)\n",
    "azdias_reduced = pca.fit_transform(azdias_scaled)\n",
    "print('azdias done!')\n",
    "customers_reduced = pca.transform(customers_scaled)\n",
    "print('Explained variance ratio of azdias DF', pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('New shape of azdias', azdias_reduced.shape)\n",
    "print('New shape of customers', customers_reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(21,7))\n",
    "plt.plot(pca.explained_variance_ratio_.cumsum())\n",
    "plt.title('Cumulative explained variance')\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Choose an optimal # of clusters\n",
    "# With help of \"How to determine the optimal number of clusters for k-means clustering\"\n",
    "# https://blog.cambridgespark.com/how-to-determine-the-optimal-number-of-clusters-for-k-means-clustering-14f27070048f\n",
    "\n",
    "sum_of_squared_distances = []\n",
    "for k in range(2,31):\n",
    "    print('Fit {} clusters'.format(k))\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans = kmeans.fit(azdias_reduced)\n",
    "    sum_of_squared_distances.append(kmeans.inertia_) # SSD of samples to their closest cluster center"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(21,7))\n",
    "plt.plot(range(2,31), sum_of_squared_distances)\n",
    "plt.xlabel('Number of clusters, k')\n",
    "plt.ylabel('Sum of squared distances')\n",
    "plt.title('Elbow Method For Optimal k, SSE', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's very difficult to determine the optimal # of clusters from this figure. So, the further analysis of 6 and 14 clusters will be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(df, n_clusters):\n",
    "    df = pd.DataFrame(df)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters).fit(df)\n",
    "                \n",
    "    cluster_map = pd.DataFrame()\n",
    "    cluster_map['cluster'] = kmeans.labels_\n",
    "\n",
    "    return kmeans, cluster_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_azdias_6, cluster_map_6 = kmeans(azdias_reduced, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "preds_azdias_6 = pd.DataFrame(kmeans_azdias_6.predict(azdias_reduced), columns=['AZDIAS'])\n",
    "preds_customers_6 = pd.DataFrame(kmeans_azdias_6.predict(customers_reduced), columns=['CUSTOMERS'])\n",
    "\n",
    "preds_6 = pd.concat([preds_azdias_6['AZDIAS'].value_counts(), preds_customers_6['CUSTOMERS'].value_counts()],\n",
    "                                                     axis=1, sort=False)\n",
    "\n",
    "preds_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_azdias_6.shape, preds_customers_6.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "preds.plot(kind='bar', figsize=(21,7), width=.9)\n",
    "plt.title('Clusters of general population of Germany vs. customers of a mail-order company (6 clusters)', fontsize=15)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### k=14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_azdias_14, cluster_map_14 = kmeans(azdias_reduced, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "preds_azdias_14 = pd.DataFrame(kmeans_azdias_14.predict(azdias_reduced), columns=['AZDIAS'])\n",
    "preds_customers_14 = pd.DataFrame(kmeans_azdias_14.predict(customers_reduced), columns=['CUSTOMERS'])\n",
    "\n",
    "preds_14 = pd.concat([preds_azdias_14['AZDIAS'].value_counts(), preds_customers_14['CUSTOMERS'].value_counts()],\n",
    "                                                     axis=1, sort=False)\n",
    "\n",
    "preds_14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_azdias_14.shape, preds_customers_14.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "preds.plot(kind='bar', figsize=(21,7), width=.9)\n",
    "plt.title('Clusters of general population of Germany vs. customers of a mail-order company (14 clusters)', fontsize=15)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discuss results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the prepared demographic data of the general population of Germany and customers, the use of 6 as well as 14 clusters leads to the fact that a disproportion appears in the results, and some of the people from the customers simply do not fall into the corresponding clusters of the entire population.\n",
    "\n",
    "However, the most important factor is business request. It include an amount of resources the company would spend, target audience (for example, online purchases of only one product of cosmetics, etc.). In this analysis I use results of segmentation into 6 and 14 clusters, but it can still be any number of clusters. Thus, significantly reducing costs and not covering all customers. And of course, further research needs to look at the results of hierarchical clustering in order to compare several clustering methods. This analysis will take more resources, including time, than KMeans clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_6['AZDIAS'] = round(preds_6['AZDIAS'] / preds_6['AZDIAS'].sum() * 100, 1)\n",
    "preds_6['CUSTOMERS'] = round(preds_6['CUSTOMERS'] / preds_6['CUSTOMERS'].sum() * 100, 1)\n",
    "preds_6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_14['AZDIAS'] = round(preds_14['AZDIAS'] / preds_14['AZDIAS'].sum() * 100, 1)\n",
    "preds_14['CUSTOMERS'] = round(preds_14['CUSTOMERS'] / preds_14['CUSTOMERS'].sum() * 100, 1)\n",
    "preds_14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Shape extra customers DF' + str(customers_extra.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze 6 clusters scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "preds_customers_6 = pd.concat([preds_customers_6, customers_extra], axis=1, sort=False)\n",
    "print('Shape (6 clusters)', preds_customers_6.shape)\n",
    "preds_customers_6.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "round(pd.crosstab(preds_customers_6['CUSTOMERS'], preds_customers_6['CUSTOMER_GROUP'])\n",
    "      / preds_customers_6.shape[0] * 100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "round(pd.crosstab(preds_customers_6['CUSTOMERS'], preds_customers_6['ONLINE_PURCHASE'])\n",
    "      / preds_customers_6.shape[0] * 100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "round(pd.crosstab(preds_customers_6['CUSTOMERS'], preds_customers_6['PRODUCT_GROUP'])\n",
    "      / preds_customers_6.shape[0] * 100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_customers_6['ONLINE_PURCHASE'] = preds_customers_6['ONLINE_PURCHASE'].replace({'Yes': 1, 'No': 0})\n",
    "round(pd.pivot_table(data=preds_customers_6, index='CUSTOMERS', columns='CUSTOMER_GROUP', values='ONLINE_PURCHASE',\n",
    "               aggfunc='mean') * 100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "round(pd.pivot_table(data=preds_customers_6, index='CUSTOMERS', columns='PRODUCT_GROUP', values='ONLINE_PURCHASE',\n",
    "               aggfunc='mean') * 100, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze 14 clusters scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_customers_14 = pd.concat([preds_customers_14, customers_extra], axis=1, sort=False)\n",
    "print('Shape (14 clusters)', preds_customers_14.shape)\n",
    "preds_customers_14.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "round(pd.crosstab(preds_customers_14['CUSTOMERS'], preds_customers_14['CUSTOMER_GROUP'])\n",
    "      / preds_customers_14.shape[0] * 100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "round(pd.crosstab(preds_customers_14['CUSTOMERS'], preds_customers_14['ONLINE_PURCHASE'])\n",
    "      / preds_customers_14.shape[0] * 100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "round(pd.crosstab(preds_customers_14['CUSTOMERS'], preds_customers_14['PRODUCT_GROUP'])\n",
    "      / preds_customers_14.shape[0] * 100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds_customers_14['ONLINE_PURCHASE'] = preds_customers_14['ONLINE_PURCHASE'].replace({'Yes': 1, 'No': 0})\n",
    "round(pd.pivot_table(data=preds_customers_14, index='CUSTOMERS', columns='CUSTOMER_GROUP', values='ONLINE_PURCHASE',\n",
    "               aggfunc='mean') * 100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "round(pd.pivot_table(data=preds_customers_14, index='CUSTOMERS', columns='PRODUCT_GROUP', values='ONLINE_PURCHASE',\n",
    "               aggfunc='mean') * 100, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Supervised Learning Model\n",
    "\n",
    "Now that you've found which parts of the population are more likely to be customers of the mail-order company, it's time to build a prediction model. Each of the rows in the \"MAILOUT\" data files represents an individual that was targeted for a mailout campaign. Ideally, we should be able to use the demographic information from each individual to decide whether or not it will be worth it to include that person in the campaign.\n",
    "\n",
    "The \"MAILOUT\" data has been split into two approximately equal parts, each with almost 43 000 data rows. In this part, you can verify your model with the \"TRAIN\" partition, which includes a column, \"RESPONSE\", that states whether or not a person became a customer of the company following the campaign. In the next part, you'll need to create predictions on the \"TEST\" partition, where the \"RESPONSE\" column has been withheld."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mailout_train = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_MAILOUT_052018_TRAIN.csv', sep=';')\n",
    "mailout_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Shape', mailout_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y = mailout_train['RESPONSE']\n",
    "X = mailout_train.drop('RESPONSE', axis=1)\n",
    "X = clean_demographic_data(X, unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'Shape X', X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('xtick', labelsize=12)\n",
    "\n",
    "plt.subplots(figsize=(21,7))\n",
    "y.value_counts().iloc[:20].plot(kind='bar', width=0.9, cmap='GnBu_r')\n",
    "plt.title('Response', fontsize=15)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "        ('scale', scaler),\n",
    "        ('reduce_dim', pca),\n",
    "        ('clf', RandomForestClassifier(class_weight='balanced'))\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train pipeline\n",
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = pipeline.predict(X_test)\n",
    "print(pred.shape, y_test.shape)\n",
    "\n",
    "precision_ = precision_score(y_test, pred)\n",
    "recall_ = recall_score(y_test, pred)\n",
    "f1_ = f1_score(y_test, pred)\n",
    "accuracy_ = accuracy_score(y_test, pred)\n",
    "\n",
    "precision_, recall_, f1_, accuracy_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.mean(), y_test.mean(), pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use grid search to find better parameters\n",
    "\n",
    "parameters = {\n",
    "        'clf__n_estimators': [50, 100, 200],\n",
    "        'criterion': ['gini', 'entropy'],\n",
    "        'max_features': ['auto', 'sqrt', 'log2', None],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_2 = cv.predict(X_test)\n",
    "print(pred_2.shape, y_test.shape)\n",
    "\n",
    "precision_ = precision_score(y_test, pred_2)\n",
    "recall_ = recall_score(y_test, pred_2)\n",
    "f1_ = f1_score(y_test, pred_2)\n",
    "accuracy_ = accuracy_score(y_test, pred_2)\n",
    "\n",
    "precision_, recall_, f1_, accuracy_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Kaggle Competition\n",
    "\n",
    "Now that you've created a model to predict which individuals are most likely to respond to a mailout campaign, it's time to test that model in competition through Kaggle. If you click on the link [here](http://www.kaggle.com/t/21e6d45d4c574c7fa2d868f0e8c83140), you'll be taken to the competition page where, if you have a Kaggle account, you can enter. If you're one of the top performers, you may have the chance to be contacted by a hiring manager from Arvato or Bertelsmann for an interview!\n",
    "\n",
    "Your entry to the competition should be a CSV file with two columns. The first column should be a copy of \"LNR\", which acts as an ID number for each individual in the \"TEST\" partition. The second column, \"RESPONSE\", should be some measure of how likely each individual became a customer – this might not be a straightforward probability. As you should have found in Part 2, there is a large output class imbalance, where most individuals did not respond to the mailout. Thus, predicting individual classes and using accuracy does not seem to be an appropriate performance evaluation method. Instead, the competition will be using AUC to evaluate performance. The exact values of the \"RESPONSE\" column do not matter as much: only that the higher values try to capture as many of the actual customers as possible, early in the ROC curve sweep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailout_test = pd.read_csv('../../data/Term2/capstone/arvato_data/Udacity_MAILOUT_052018_TEST.csv', sep=';')\n",
    "mailout_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mailout_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lnr = mailout_test['LNR']\n",
    "mailout_test_cleaned = clean_demographic_data(mailout_test, unknown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_3 = pipeline.predict(mailout_test_cleaned)\n",
    "pred_4 = cv.predict(mailout_test_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arvato_capstone_submission_3 = pd.DataFrame({'LNR': lnr, 'RESPONSE': pred_3})\n",
    "arvato_capstone_submission_4 = pd.DataFrame({'LNR': lnr, 'RESPONSE': pred_4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arvato_capstone_submission_3.to_csv('Arvato_Capstone_Submission_3.csv', index=False)\n",
    "arvato_capstone_submission_4.to_csv('Arvato_Capstone_Submission_4.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
